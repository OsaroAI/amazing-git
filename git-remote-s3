#!/usr/bin/env python

import logbook

from dulwich import pack
from dulwich.object_store import DiskObjectStore
from dulwich.repo import Repo, BaseRepo

from boto.s3.connection import S3Connection
from boto.exception import S3ResponseError

from gitutil import GitRemoteHandler, parse_s3_url, HandlerException, merge_git_config

from dulwich_s3 import S3Repo

log = logbook.Logger('git-remote-s3')

def get_from_sections(conf, sections, key):
	log.debug('Loading %s from sections %r' % (key, sections))
	for section in sections:
		if section in conf and key in conf[section]:
			return conf[section][key]


class S3Handler(GitRemoteHandler):
#	supported_options = ['dry-run']
	# FIXME: use fallback to use smart protocol for what we can actually push?

	# lazy attributes, instantiate when we need them
	_bucket = None
	_remote_repo = None

	def __init__(self, *args, **kwargs):
		super(S3Handler, self).__init__(*args, **kwargs)

		# parse URL
		remote_s3 = parse_s3_url(self.remote_address)
		log.debug('Remote URL %s parsed to %s' % (self.remote_address, remote_s3))

		# load git configuration
		conf = merge_git_config()

		# the sections in gut config files we are reading
		conf_sections = ['remote "%s"' % self.remote_name, 's3']

		self.remote_bucket = remote_s3['bucket']
		self.remote_prefix = remote_s3['prefix'] or ''
		self.remote_key = remote_s3['key'] or get_from_sections(conf, conf_sections, 'key')
		if not self.remote_key: HandlerException('No access key specified. Cannot access bucket %s' % self.remote_bucket)

		# load a secret for a specific key
		self.remote_secret = remote_s3['secret'] or get_from_sections(conf, conf_sections, '%s-secret' % self.remote_key)
		if not self.remote_secret: raise HandlerException('No secret key specified. Cannot access bucket %s' % self.remote_bucket)

		log.debug('loaded credentials, final url: http://%s:%s@%s:%s' % (self.remote_key, self.remote_secret, self.remote_bucket, self.remote_prefix))

	@property
	def bucket(self):
		try:
			if not self._bucket:
				conn = S3Connection(self.remote_key, self.remote_secret)
				log.debug('Opened S3Connection %r' % conn)
				self._bucket = conn.get_bucket(self.remote_bucket)
				log.debug('Got bucket: %r' % self._bucket)
		except S3ResponseError, e:
			if 'InvalidAccessKeyId' == e.error_code: raise HandlerException('S3: Unknown access key: "%s"' % self.remote_key)
			if 'SignatureDoesNotMatch' == e.error_code: raise HandlerException('S3: Signature mismtach: Possibly wrong secret key for access key "%s"' % self.remote_key)
			if 'NoSuchBucket' == e.error_code: raise HandlerException('S3: No such bucket: %s' % self.remote_bucket)
			raise HandlerException('S3: Error: %s' % e.error_message)

		return self._bucket

	@property
	def remote_repo(self):
		if not self._remote_repo:
			self._remote_repo = S3Repo(self.bucket, self.remote_prefix)
			log.debug('Instantiated repo: %r' % self._remote_repo)

		return self._remote_repo

	def git_list(self, *args):
		log.debug('listing refs')
		for name, hash in self.remote_repo.get_refs().iteritems():
			log.debug('ref found: %s %s' % (name, hash))
			print '%s %s' % (hash, name)
		print

	def git_push(self, target):
		log.debug('push args: %s' % target)
		src, dst = target.split(':')
		log.debug('push: %s to %s' % (src, dst))

		# the local repository
		local = Repo('.git')

		# "push" == we use .fetch() to "fetch" from the local TO the remote ("target"),
		# then update the refs
		def determine_wants(heads):
			wants = [ heads[src] ]
			log.debug('pushing %r, wants is using %r' % (src, wants))
			return wants

		log.debug('calling fetch')
		local.fetch(self.remote_repo, determine_wants, self.report_progress)

		# uploaded everything, update refs next
		# FIXME: Update refs

		# report pushing went okay
		print "ok %s" % dst
		print

	def report_progress(self, msg):
		log.info(msg)


if __name__ == '__main__':
	try:
		loghandler = logbook.FileHandler('git-remote-test.log', encoding = 'utf-8')
		S3Handler().run()
	except HandlerException, e:
		log.critical(e)
	except Exception, e:
		log.exception(e)
